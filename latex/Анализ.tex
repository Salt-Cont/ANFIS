\section{Анализ предметной области}
\subsection{Понятие искусственной нейронной сети}
Нейронная сеть, также именуемая искусственной нервной сетью (ИНС), это математическая модель, которая послужила программным или аппаратным воплощением принципов функционирования биологических нейронных сетей - сетей нервных клеток живых организмов. Этот термин возник в результате изучения процессов, происходящих в мозге, и попыток эмулировать их. Первые усилия по созданию нейронных сетей были предприняты У. Маккалоком и У. Питтсом. С развитием алгоритмов обучения этих моделей стали широко применяться в практических задачах, таких как предсказание, распознавание образов, управление и прочие.

Искусственная нейронная сеть (ИНС) - это архитектура, в которой простые вычислительные элементы, называемые искусственными нейронами, взаимодействуют между собой для выполнения сложных задач. Каждый нейрон обрабатывает входные сигналы и передает выходные сигналы другим нейронам в сети. Хотя отдельные нейроны могут быть простыми, их коллективное взаимодействие в большой сети позволяет выполнять сложные вычисления и задачи.

Искусственная нейронная сеть (ИНС) представляет собой комплекс систематически связанных и взаимодействующих между собой простых вычислительных элементов, называемых искусственными нейронами. В контексте машинного обучения, они функционируют как специализированный вид методов для распознавания образов и дискриминантного анализа. Математически они представляют собой сложную многопараметрическую задачу нелинейной оптимизации. В кибернетике они используются для адаптивного управления и в робототехнике. С точки зрения развития вычислительной техники и программирования, они являются мощным инструментом для решения проблемы эффективного параллелизма.

Важно отметить, что нейронные сети не программированны в традиционном понимании этого термина; они обучаются. Процесс обучения заключается в настройке параметров связей между нейронами на основе предоставленных данных. Этот процесс позволяет нейронным сетям обнаруживать сложные закономерности в данных, выполнять обобщение и возвращать верные результаты даже на основе данных, которые не были представлены в процессе обучения или были представлены с искажениями.

\subsubsection{Распознавание образов и классификация}
Когда мы говорим о различных "образах", мы имеем в виду разнообразные объекты, такие как текстовые символы, изображения, звуковые образцы и так далее. В процессе обучения нейронной сети предъявляются эти различные образы, каждому из которых присваивается определенный класс. Образец представляется в виде вектора значений признаков, и сеть учится определять, к какому классу относится каждый образец. Важно, чтобы набор признаков однозначно определял класс образца. Если признаков недостаточно, сеть может неправильно классифицировать образец, связывая его с несколькими классами.

После завершения обучения количество нейронов в выходном слое сети обычно соответствует количеству классов. Каждый нейрон в выходном слое представляет определенный класс, и сеть выдаёт ответ о принадлежности образца к тому или иному классу. Если сеть уверена в классификации, на одном из выходов появится признак принадлежности к классу, а на других выходах этот признак отсутствует. Однако, если сеть не уверена, может возникнуть ситуация, когда на нескольких выходах присутствует признак принадлежности к классу, что указывает на неопределённость сети в своём ответе.

\subsubsection{Образы и Классификация}
При обучении нейронной сети различные типы данных, такие как текст, изображения и звук, представлены в виде образов и привязаны к определенным классам. Сеть изучает эти образы и их признаки, чтобы точно классифицировать их. Важно, чтобы эти признаки явно указывали на класс образца. По завершении обучения количество нейронов в выходном слое сети соответствует количеству классов. Каждый нейрон представляет определенный класс, и сеть выдает ответ о принадлежности образца к определенному классу. В случае неопределенности сеть может указать на несколько возможных классов.

Классификация по формату входной информации:
\begin{enumerate}
\item Аналоговые нейронные сети: работают с информацией в форме действительных чисел.
\item Двоичные нейронные сети: оперируют с информацией, представленной в двоичном виде.
\item Образные нейронные сети: оперируют с информацией, представленной в виде образов, таких как знаки, иероглифы или символы.
\end{enumerate}

Классификация по типу обучения:
\begin{enumerate}
\item Обучение с учителем: Нейронная сеть использует известные пары входных данных и соответствующих им выходных значений для обучения. В этом случае выходное пространство решений известно заранее.
\item Обучение без учителя: Нейронная сеть формирует выходное пространство решений только на основе входных данных, без предоставления соответствующих выходных значений. Такие сети называют самоорганизующимися, так как они обнаруживают структуры или паттерны в данных без явного учителя.
\item Обучение с подкреплением: В этом типе обучения система взаимодействует со средой, принимая последовательность действий и получая за них награды или наказания. Цель состоит в том, чтобы оптимизировать стратегию действий так, чтобы максимизировать суммарную награду в долгосрочной перспективе.
\end{enumerate}

Существует два типа классификации синапсов по характеру настройки:
\begin{enumerate}
\item  Сети с фиксированными связями: Весовые коэффициенты нейронной сети выбираются заранее и остаются неизменными на протяжении работы сети. Это означает, что они не подвергаются изменениям в процессе обучения и задаются исходными условиями задачи.
\item Сети с динамическими связями: В этих сетях весовые коэффициенты синапсов настраиваются в процессе обучения. Это позволяет сети адаптироваться к новой информации и улучшать свою производительность в зависимости от задачи или окружающей среды.
 \item Обучение с подкреплением: В этом типе обучения система взаимодействует со средой, принимая последовательность действий и получая за них награды или наказания. Цель состоит в том, чтобы оптимизировать стратегию действий так, чтобы максимизировать суммарную награду в долгосрочной перспективе.
\end{enumerate}

Классификация по характеру связей в нейронных сетях
\begin{enumerate}
\item   Нейронные сети прямого распространения:

	Все связи направлены строго от входных нейронов к выходным.

	Примеры: перцептрон Розенблатта, многослойный перцептрон, сети Ворда.

\item Рекуррентные нейронные сети:

	Сигнал с выходных нейронов или нейронов скрытого слоя частично передаётся обратно на входы нейронов входного слоя (обратная связь).
	
	Рекуррентная сеть Хопфилда решает задачи компрессии данных и построения ассоциативной памяти.
	
	Частный случай: двунаправленные сети.
	
 \item Радиально-базисные функции (RBF):

	Используются нейронные сети с единственным скрытым слоем.
	
	Нелинейная активационная функция только у нейронов скрытого слоя.
	
	Синаптические веса связей входного и скрытого слоёв равны единице.
\item Самоорганизующие карты (Self-Organizing Maps, SOM), представляют собой модель нейронных сетей, которая используется для визуализации и кластеризации данных. Это метод проецирования данных из многомерного пространства в пространство с более низкой размерностью, обычно двумерное. SOM также применяются в моделировании, прогнозировании и других задачах. Они являются разновидностью сетей Кохонена.

В сети Кохонена сигнал поступает на все нейроны одновременно, а выходной сигнал формируется по принципу "победитель забирает всё". В процессе обучения веса синапсов настраиваются таким образом, чтобы узлы сети описывали кластерную структуру данных. Удобно представлять SOM как двумерную сетку узлов в многомерном пространстве.

Начальное вложение сетки в пространство данных выбирается произвольно, а затем узлы сети перемещаются на каждом этапе обучения в направлении данных. Алгоритм обучения состоит из двух этапов: грубой настройки, где узлы двигаются коллективно для грубого отображения структуры данных, и тонкой настройки, где настраиваются индивидуальные положения узлов.

Этот процесс повторяется определённое число эпох, причем количество шагов может изменяться в зависимости от задачи.
\end{enumerate}

\subsubsection{Кластеризация и Новые Классы}
Кластеризация подразумевает разделение входных сигналов на классы, неизвестные заранее по числу и признакам. После обучения сеть может определить, к какому классу относится входной сигнал, либо указать на его новизну. Такие сети могут обнаруживать новые, ранее неизвестные классы сигналов. Соответствие между выделенными сетью классами и классами в предметной области устанавливается человеком.

\subsubsection{Архитектура Нейронных Сетей}
Нейронные сети Кохонена имеют ограниченный размер, разделяясь на гиперслои и ядра. Идеальное количество параллельных слоев ограничено до 112, где каждый слой содержит от 500 до 2000 микроколонок. Эти микроколонки обеспечивают кодирование и вывод результатов. Регулирование числа нейронов и слоев осуществляется с помощью суперкомпьютеров, делая нейронные сети пластичными и адаптивными.

\subsubsection{Прогнозирование}
Способность нейронной сети к прогнозированию происходит из ее способности к обобщению и выявлению скрытых зависимостей между входными и выходными данными. После обучения сеть может предсказать будущее значение последовательности, основываясь на предыдущих значениях и/или текущих факторах. Прогнозирование возможно лишь в случае, если предыдущие изменения в некоторой степени влияют на будущие. Например, прогнозирование цен акций на основе предыдущих недельных котировок может быть успешным (но не обязательно), в то время как прогнозирование результатов лотереи на основе 50-летних данных практически бесполезно.

\subsubsection{Аппроксимация}
Способность нейронной сети к прогнозированию напрямую вытекает из ее способности обобщать и выявлять скрытые зависимости между входными и выходными данными. После обучения сеть может предсказывать будущее значение последовательности, опираясь на предыдущие значения и/или текущие факторы. Прогнозирование возможно только в том случае, если предыдущие изменения в некоторой степени определяют будущие. Например, прогнозирование цен акций на основе предыдущих недельных котировок может быть успешным (но не обязательно), в то время как прогнозирование результатов лотереи на основе 50-летних данных практически бесполезно.

Нейронные сети способны аппроксимировать непрерывные функции. Обобщённая аппроксимационная теорема показывает, что с помощью линейных операций и каскадного соединения можно получить устройство, способное вычислить любую непрерывную функцию с некоторой наперёд заданной точностью. Это означает, что нейроны могут иметь различные нелинейные характеристики, от сигмоидальной до волновых пакетов или вейвлетов, синусов или многочленов. Выбор нелинейной функции может влиять на сложность сети, но при правильном выборе структуры нейронная сеть остаётся универсальным аппроксиматором и может достаточно точно аппроксимировать функционирование любого непрерывного автомата.

\subsubsection{Сжатие данных и ассоциативная память}
Нейросети обладают способностью выявлять взаимосвязи между различными параметрами, что позволяет более компактно представлять данные большой размерности в случае их тесной взаимосвязи. Процесс обратного восстановления исходного набора данных из части информации называется (авто)ассоциативной памятью. Ассоциативная память также способна восстанавливать исходный сигнал или образ из зашумленных или повреждённых входных данных. Решение задачи гетероассоциативной памяти позволяет реализовать память, адресуемую по содержимому.

\subsection{Понятие нечёткой логики}
Нечеткая логика представляет собой раздел математики, который расширяет традиционную логику и теорию множеств, используя концепцию нечетких множеств. Она была впервые предложена Лотфи Заде в 1965 году. В отличие от классической логики, где элементы либо принадлежат множеству (имеют значение 1), либо не принадлежат (имеют значение 0), нечеткие множества могут иметь значения на интервале [0, 1], отражая степень принадлежности элемента к множеству. На основе этой концепции разрабатываются различные логические операции и определяются лингвистические переменные, значениями которых являются нечеткие множества.

Область применения нечеткой логики включает исследование рассуждений в условиях нечеткости, размытости, аналогичных рассуждениям в обычной логике, а также их применение в вычислительных системах.

\subsubsection{Символическая нечёткая логика}
Нечёткая логика, также известная как символическая нечёткая логика, базируется на концепции t-нормы. После выбора определённой t-нормы появляется возможность определить основные операции над пропозициональными переменными: конъюнкцию, дизъюнкцию, импликацию, отрицание и другие.

Теорема о дистрибутивности, свойственная классической логике, выполняется лишь при использовании t-нормы Гёделя. Импликация обычно определяется операцией, называемой residium, которая, в свою очередь, зависит от выбранной t-нормы.

Эти базовые операции приводят к формальному определению базовой нечёткой логики, имеющей сходства с классической булевой логикой (исчислением высказываний).

Существуют три основные базовые нечёткие логики: логика Лукасевича, логика Гёделя и вероятностная логика. Интересно, что объединение любых двух из этих логик приводит к классической булевой логике.

\subsubsection{Синтез функций непрерывной логики заданных таблично}
Функция нечёткой логики Заде всегда принимает значение одного из своих аргументов либо его отрицания. Таким образом, функцию нечёткой логики можно задать таблицей выбора, в которой перечислены все варианты упорядочения аргументов и отрицаний, и для каждого варианта указано значение функции.

Однако не любая произвольная таблица выбора задаёт функцию нечёткой логики. В одной работе был сформулирован критерий, позволяющий определить, является ли функция, заданная таблицей выбора, функцией нечёткой логики, и предложен простой алгоритм синтеза, основанный на концепциях конституента минимума и максимума. Функция нечёткой логики представляет собой дизъюнкцию конституент минимума, где конституента максимума — это конъюнкция переменных текущей области, больших либо равных значению функции в этой области (справа от значения функции в неравенстве, включая значение функции).

\subsubsection{Теория приближённых вычислений}
В общем, основное понятие нечёткой логики можно описать как использование нечётких множеств, которые определяются через обобщённую характеристическую функцию. Это позволяет работать с нечёткими отношениями, объединениями, пересечениями и дополнениями множеств. Важным элементом является лингвистическая переменная.

Для применения нечёткой логики в некоторых сферах достаточно этого минимального набора определений. Однако для большинства случаев требуется также определить правила вывода и оператор импликации.

\subsubsection{Нечёткая логика и нейронные сети}
Основная идея нечёткой логики в широком смысле заключается в использовании нечётких множеств, которые определяются через обобщённую характеристическую функцию. Путем введения операций объединения, пересечения и дополнения множеств (через характеристическую функцию) а также концепции нечётких отношений и лингвистических переменных, создается база для приложения нечёткой логики в различных областях.

Важно отметить, что для некоторых применений этого подхода достаточно указанных минимальных определений. Однако для большинства случаев необходимо определить правила вывода и оператор импликации.

Кроме того, поскольку нечёткие множества могут быть представлены функциями принадлежности, а t-нормы и k-нормы являются обычными математическими операциями, возможно представление нечётких логических рассуждений в виде нейронной сети. Здесь функции принадлежности интерпретируются как функции активации нейронов, передача сигналов как связи, а логические t-нормы и k-нормы как специальные виды нейронов, выполняющие соответствующие математические операции. Существует разнообразие подобных нейро-нечётких сетей, включая ANFIS (Adaptive Neuro Fuzzy Inference System) - адаптивную нейро-нечеткую систему вывода. О ней чуть расскажу чуть позже.

\subsubsection{Байесовская вероятность}
Связь между нечёткой логикой и байесовской вероятностью выражается через байесовскую логико-вероятностную модель нечёткого вывода. Эта модель трансформирует нечёткие продукции в вероятностные функции, определяющие апостериорное распределение на множестве гипотез, соответствующих значениям выходной лингвистической переменной. После этого происходит дефаззификация для получения чёткого значения выходной переменной.

Неонечтокие продукции состоят из правил типа "ЕСЛИ ..., ТО ...", где посылка и заключение содержат лингвистические переменные, а лингвистические переменные имеют терм-множества с функциями принадлежности.

Байесовская логико-вероятностная модель нечёткого вывода рассматривает значения выходной переменной как гипотезы и определяет их априорные вероятности. Новая информация поступает в виде значений входных переменных, которые служат свидетельствами в пользу или против гипотез.

Условные вероятности заключений при данных посылках определяются через функции принадлежности нечётких множеств.

Используя теорему Байеса, априорные вероятности обновляются с учетом новой информации, что позволяет получить апостериорное распределение вероятностей на множестве гипотез.

Для дефаззификации можно использовать различные методы, такие как метод максимума апостериорной вероятности (MAP) или метод среднего значения апостериорной вероятности (MEP).

\subsection{Понятие Адаптивной системы нейро-нечеткого вывода}
Адаптивная система нейро-нечеткого вывода (ANFIS) представляет собой разновидность искусственной нейронной сети, основанной на системе нечеткого вывода Такаги-Сугено. Разработанная в начале 1990-х годов, эта методика объединяет в себе нейронные сети и принципы нечеткой логики, что дает ей потенциал для использования преимуществ обоих в одной структуре.

В ANFIS система вывода соответствует набору нечетких правил "ЕСЛИ–ТО", способных к обучению для аппроксимации нелинейных функций. Следовательно, ANFIS считается универсальным оценщиком. Для более эффективного и оптимального использования ANFIS можно воспользоваться наилучшими параметрами, полученными с использованием генетического алгоритма. Эта технология находит применение в интеллектуальных системах управления энергопотреблением.

ANFIS представляет собой разновидность искусственной нейронной сети, базирующейся на нечеткой системе вывода Такаги-Сугено. Разработанная в начале 1990-х годов, эта методика объединяет в себе нейронные сети и принципы нечеткой логики, что дает ей потенциал для использования преимуществ обоих в одной структуре.

Архитектура ANFIS
В структуре сети можно выделить две части: исходную и последующую. Архитектура состоит из пяти уровней:
\begin{enumerate}
\item Уровень фаззификации: Принимает входные значения и определяет функции принадлежности, принадлежащие им. Степени принадлежности вычисляются на основе исходных параметров.
\item Уровень правил: Генерирует сильные стороны для правил на основе вторичных параметров.
\item Уровень нормализации: Нормализует вычисленную силу срабатывания путем деления каждого значения на общую силу срабатывания.
\item Уровень следствия: Принимает нормализованные значения и параметры следствия. Возвращает дефаззифицированные значения.
\item Выходной уровень: Получает дефаззифицированные значения и возвращает конечный результат.
\end{enumerate}

ANFIS позволяет аппроксимировать нелинейные функции, делая его универсальным оценщиком с использованием генетического алгоритма для оптимизации его параметров. Эта технология находит применение в интеллектуальных системах управления энергопотреблением.

\subsubsection{Слой фаззификации}
Первый уровень в сети ANFIS представляет собой ключевое отличие от обычной нейронной сети. Обычно нейронные сети работают с этапом предварительной обработки данных, на котором признаки преобразуются в нормализованные значения от 0 до 1. Однако в ANFIS нет необходимости в использовании сигмоидальной функции. Вместо этого происходит преобразование числовых значений в нечеткие.

Давайте рассмотрим пример: предположим, у нас есть сеть, которая получает на вход расстояние между двумя точками в 2D-пространстве. Это расстояние измеряется в пикселях и может принимать значения от 0 до 500 пикселей. Преобразование числовых значений в нечеткие выполняется с использованием функций принадлежности, которые состоят из семантических описаний, таких как "близко", "средне" и "дальше". Каждое из этих лингвистических значений соответствует отдельному нейрону. Например, нейрон "близко" срабатывает с некоторым значением от 0 до 1, если расстояние попадает в категорию "близко". Точно так же нейрон "средне" срабатывает, если расстояние соответствует этой категории. Таким образом, входное значение "расстояние в пикселях" разбивается на три разных нейрона для каждой категории: "близко", "средне" и "дальше".
